# Mask & compute seasonal average then write out data.
#
# 1) Mask where BBF > 0.5
# 2) Mask (per month) where count_raw_Reflectivity_thresh > 20% of  samples
# 3) Mask (per month) where number of raw samples < 70% of max samples
# 4) Also mask out where over ocn.


import xarray
import wradlib as wrl
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import typing
import pandas as pd
import pathlib
import ast

import ausLib

logger = ausLib.my_logger
ausLib.init_log(logger, level='DEBUG')
horizontal_coords = ['x', 'y']  # for radar data.
cpm_horizontal_coords = ['grid_latitude', 'grid_longitude']  # horizontal coords for CPM data.



def group_data_set(ds: xarray.Dataset, *args, group_dim:str = 'time',**kwargs) -> xarray.Dataset:
    """

    Args:
        ds: dataset
        group_dim: dimension to group over

    Returns: grouped dataset for variables that start with:
        max_  -- compute the max.
        mean_ -- compute the mean.
        count_ -- sum the variable.
        time_max_ -- take argmax of the equiv max_ var and then use that index.
        sample_resolution -- take the average.
        fract_ compute the mean
        time_bounds -- take the min and max values.
    Anything else -- just take the first one.

    """
    time_str = ds.time[[0, -1]].dt.strftime('%Y-%m-%d').values
    logger.info(f"Grouping over {group_dim} with args: {args} & kwargs: {kwargs} for time: {time_str} ")
    result = dict()
    if ds.max_reflectivity.notnull().sum() == 0:
        logger.warning(f"No valid values for max_reflectivity at {time_str}")
        return xarray.Dataset()
    with xarray.set_options(keep_attrs=True):
        for vname, da in ds.items():
            if vname.startswith('max_'):
                da_group = da.max(group_dim)
                logger.debug(f"Computed max for {vname}")
            elif vname.startswith('mean_'):
                da_group = da.mean(group_dim)
                logger.debug(f"Computed mean for {vname}")
            elif vname.startswith('count_'):
                da_group = da.sum(group_dim)
                logger.debug(f"Computed sum for {vname}")
            elif vname.startswith('time_max_'):
                max_name = vname.replace('time_max_', 'max_')
                indx = ds[max_name].idxmax(group_dim).compute()
                L = indx.notnull()
                if L.sum() == 0:
                    logger.warning(f"No valid values for {vname}")
                    continue
                indx = indx.where(L, da[group_dim].min())
                da_group = da.sel({group_dim: indx}).compute().where(L, np.datetime64('NaT'))
                logger.debug(f"Computed time_max for {vname}")
            elif vname.startswith('median_'):
                da_group = da.median(group_dim)
                logger.debug(f"Computed median for {vname}")
            elif vname.startswith('fract_'):
                da_group = da.mean(group_dim)
                logger.debug(f"Computed mean for {vname}")
            elif ('_fract' in vname) or ('_fract_' in vname):
                logger.warning('Using old fract_ naming. Reprocess')
                da_group = da.mean(group_dim)
                logger.debug(f"Computed mean for {vname}")
            elif vname in  ['sample_resolution','max_fraction','threshold']:
                da_group = da.mean(group_dim)
                logger.debug(f"Computed mean for {vname}")
            elif vname == 'time_bounds':
                da = da.load()
                da_group = xarray.concat([da.min(),da.max()],dim='bounds')
                logger.debug(f"Computed time_bounds for {vname}")
            else:
                logger.debug(f"Taking first one for {vname}")
                da_group = da.isel({group_dim: 0})
            result[vname] = da_group.drop_vars(group_dim, errors='ignore')
    result = xarray.Dataset(result).assign_attrs(ds.attrs)
    return result


##

site = 'Melbourne'
rgn = dict(x=slice(-75e3,75e3),y=slice(-75e3,75e3))
in_radar = sorted((ausLib.data_dir/f"summary_reflectivity/{site}").glob("hist_gndrefl*.nc"))
in_BBF_DEM = ausLib.data_dir/f"ancil/{site}_cbb_dem.nc"
out_radar = ausLib.data_dir/f"summary_reflectivity/processed/{site}_hist_gndrefl_DJF.nc"
out_radar.parent.mkdir(exist_ok=True, parents=True) # make directory if it doesn't exist.
radar = xarray.open_mfdataset(in_radar)
logger.info(radar.proj.dims)
# extract the region used from the meta-data though need to reverse  y
rgn = [v for v in radar.attrs['program_args'] if v.startswith('region:')][0]
rgn=np.array(ast.literal_eval(rgn.split(':')[1])) # thanks chatgpt fot this
# convert to m from km
rgn *= 1000.
rgn = dict(x=slice(*rgn[0:2]),y=slice(*rgn[-1:-3:-1]))
CBB_DEM = xarray.load_dataset(in_BBF_DEM).sel(**rgn)
CBB_DEM = CBB_DEM.coarsen(x=4, y=4, boundary='trim').mean()
bbf = CBB_DEM.CBB.clip(0,1)
# keep where BBF < 0.5 and land.
msk = (bbf < 0.5) & (CBB_DEM.elevation > 0.0)
# msk variables which have both an x and y dimension.
xy_vars = [v for v in radar.variables if ('x' in radar[v].dims) and ('y' in radar[v].dims)]
for var in xy_vars:
    radar[var] = radar[var].where(msk)
    logger.debug(f"Masked {var}")
logger.info('Masked xy dims')


# seasonally group
resamp_prd= xarray.DataArray([pd.Timedelta(str(c)).total_seconds() for c in radar.resample_prd.values],
dims='resample_prd',
coords=dict(resample_prd=radar.resample_prd))
max_samples_resamp = radar.time.dt.days_in_month * 24 * 60 * 60 / resamp_prd
max_samples_resamp = max_samples_resamp.resample(time='QS-DEC').sum() # samples/quarter
radar = radar.resample(time='QS-DEC').map(group_data_set, group_dim='time')

logger.info('resampled')


# now mask out the seasonal data.
# and where total samples > 70% of max samples.
samples = radar.count_raw_reflectivity
max_samples = radar.time.dt.days_in_month * 24 * 60 * 60 / radar.sample_resolution.dt.seconds
fraction = samples / max_samples
tmsk = fraction > 0.7
# and where count_raw_reflectivity_thresh < 20% of samples
tmsk = tmsk & (radar.count_raw_reflectivity_thresh < 0.2 * samples)


radar = radar.compute()
# mask out the x-y vars
for var in xy_vars:
    radar[var] = radar[var].where(tmsk)
    logger.debug(f"Masked {var} for max samples")
L = radar.time.dt.season == 'DJF'  # summer in Melbourne
radar = radar.where(L, drop=True)
max_samples_resamp = max_samples_resamp.where(L, drop=True)
# apply masking on resample_prds
# and where have enough time avg samples.
fraction_resample = radar.count_reflectivity/max_samples_resamp
tmsk = fraction_resample > 0.7
x_y_resample_vars = [v for  v in radar.variables if ('x' in radar[v].dims) and ('y' in radar[v].dims) and ('resample_prd' in radar[v].dims)]
for v in x_y_resample_vars:
    radar[v] = radar[v].where(tmsk,drop=True)
    logger.debug(f"Masked {v} for resample_prd")
logger.info(radar.proj.dims)
# add in fraction_resample to the mean dataset.
radar['fraction_resample'] = fraction_resample

logger.info(radar.proj.dims)
# now  write out the data.
radar.to_netcdf(out_radar,unlimited_dims='time')